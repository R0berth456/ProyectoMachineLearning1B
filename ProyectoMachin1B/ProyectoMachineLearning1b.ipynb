{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb0bb04",
   "metadata": {},
   "source": [
    "# Proyecto Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad004033",
   "metadata": {},
   "source": [
    "#### Dependencias que se necesitan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52a67336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ffmpeg ya está instalado.\n",
      "✅ Instalación completada. Listo para usar funciones de VAD y TextGrid.\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias necesarias para detectar IPUs y generar archivos TextGrid\n",
    "\n",
    "!pip install webrtcvad praatio pydub --quiet\n",
    "\n",
    "# Descargar ffmpeg si no está instalado (solo en Colab o sistemas que lo requieran)\n",
    "import shutil\n",
    "if not shutil.which(\"ffmpeg\"):\n",
    "    print(\"ffmpeg no está instalado. Intentando instalar...\")\n",
    "    !apt-get update && apt-get install -y ffmpeg\n",
    "else:\n",
    "    print(\"✓ ffmpeg ya está instalado.\")\n",
    "\n",
    "# Confirmación de instalación\n",
    "print(\"✅ Instalación completada. Listo para usar funciones de VAD y TextGrid.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f3150",
   "metadata": {},
   "source": [
    "#### Librerias que se utilizarán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b511afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import tempfile\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "import wave\n",
    "import webrtcvad\n",
    "from difflib import SequenceMatcher\n",
    "from praatio import textgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145b341",
   "metadata": {},
   "source": [
    "### Función para transformar audios mp3 a wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55006ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Convertido: 202003311300.mp3 → 202003311300.wav\n"
     ]
    }
   ],
   "source": [
    "carpeta_audios = \"./mp3\"\n",
    "carpeta_salida_audios = \"./wav\"\n",
    "\n",
    "for archivo in os.listdir(carpeta_audios):\n",
    "    if archivo.endswith(\".mp3\"):\n",
    "        mp3_path = os.path.join(carpeta_audios, archivo)\n",
    "        nombre_base = os.path.splitext(archivo)[0]\n",
    "        wav_path = os.path.join(carpeta_salida_audios, f\"{nombre_base}.wav\")\n",
    "\n",
    "        \n",
    "        comando = f'ffmpeg -y -i \"{mp3_path}\" -ar 16000 -ac 1 \"{wav_path}\"'\n",
    "        try:\n",
    "            subprocess.run(comando, shell=True, check=True)\n",
    "            print(f\"[✓] Convertido: {archivo} → {nombre_base}.wav\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"[✗] Error al convertir {archivo}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528291e",
   "metadata": {},
   "source": [
    "### Uso de Whisper de OpenAI para transcribir segmentos de audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31928e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_segment(audio_path, start, end, model=None):\n",
    "    if model is None:\n",
    "        model = whisper.load_model(\"base\")\n",
    "\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    segment = audio[start * 1000:end * 1000] \n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp_file:\n",
    "        segment.export(tmp_file.name, format=\"wav\")\n",
    "        tmp_path = tmp_file.name\n",
    "\n",
    "    result = model.transcribe(tmp_path, language=\"es\")\n",
    "    os.remove(tmp_path)\n",
    "    return result[\"text\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8ab4a",
   "metadata": {},
   "source": [
    "#### Funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d02b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribir_audio_a_html(audio_path, output_path):\n",
    "    model = whisper.load_model(\"base\")  \n",
    "    result = model.transcribe(audio_path, language=\"es\")\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"<html><body><p>\")\n",
    "        f.write(result[\"text\"])\n",
    "        f.write(\"</p></body></html>\")\n",
    "\n",
    "    print(f\"[✓] Transcripción guardada en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4881510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_ipus(audio_path, output_textgrid):\n",
    "    try:\n",
    "        vad = webrtcvad.Vad(3)\n",
    "\n",
    "        with wave.open(audio_path, 'rb') as wf:\n",
    "            sample_rate = wf.getframerate()\n",
    "            channels = wf.getnchannels()\n",
    "            sample_width = wf.getsampwidth()\n",
    "\n",
    "          \n",
    "            assert sample_rate == 16000, \"El audio debe estar a 16kHz\"\n",
    "            assert channels == 1, \"El audio debe ser mono (1 canal)\"\n",
    "\n",
    "           \n",
    "            frame_duration_ms = 30\n",
    "            frame_size_bytes = int(sample_rate * frame_duration_ms / 1000) * sample_width\n",
    "\n",
    "            \n",
    "            frames = []\n",
    "            while True:\n",
    "                frame = wf.readframes(frame_size_bytes // sample_width)\n",
    "                if len(frame) < frame_size_bytes:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "\n",
    "        \n",
    "        voiced_segments = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            if vad.is_speech(frame, sample_rate):\n",
    "                start = i * frame_duration_ms / 1000.0\n",
    "                end = (i + 1) * frame_duration_ms / 1000.0\n",
    "                voiced_segments.append((start, end))\n",
    "\n",
    "        \n",
    "        ipus = []\n",
    "        if voiced_segments:\n",
    "            cur_start, cur_end = voiced_segments[0]\n",
    "            for start, end in voiced_segments[1:]:\n",
    "                if start - cur_end <= 0.3:  \n",
    "                    cur_end = end\n",
    "                else:\n",
    "                    ipus.append((cur_start, cur_end))\n",
    "                    cur_start, cur_end = start, end\n",
    "            ipus.append((cur_start, cur_end))\n",
    "\n",
    "        \n",
    "        entries = [(start, end, \"\") for start, end in ipus]\n",
    "        tg = textgrid.Textgrid()\n",
    "        tg.minTimestamp = 0\n",
    "        tg.maxTimestamp = ipus[-1][1] if ipus else 1.0\n",
    "        tg.addTier(textgrid.IntervalTier(\"IPU\", entries, tg.minTimestamp, tg.maxTimestamp))\n",
    "        tg.save(output_textgrid, format=\"short_textgrid\", includeBlankSpaces=True)\n",
    "\n",
    "        print(f\"[✓] TextGrid IPU guardado en: {output_textgrid}\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"[Error] Audio inválido: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Falló la detección de IPUs: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5696779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_regex(html_path):\n",
    "    with open(html_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        paragraphs = re.findall(r'<p>(.*?)</p>', content, re.DOTALL)\n",
    "        text = ' '.join(paragraphs)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c28e690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_transcription_to_ipu(ipu_intervals, audio_path):\n",
    "    \"\"\"\n",
    "    Alinea cada intervalo IPU con su transcripción usando Whisper.\n",
    "\n",
    "    Args:\n",
    "        ipu_intervals (list of tuples): Lista de (start, end, \"\") desde el TextGrid.\n",
    "        audio_path (str): Ruta al archivo .wav.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Lista de (start, end, transcripción) para cada IPU.\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"base\")\n",
    "    aligned = []\n",
    "\n",
    "    for i, (start, end, _) in enumerate(ipu_intervals):\n",
    "        duration = end - start\n",
    "\n",
    "        if duration < 0.2:\n",
    "        \n",
    "            aligned.append((start, end, \"\"))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            segment_text = transcribe_audio_segment(audio_path, start, end, model)\n",
    "            print(f\"[✓] IPU {i + 1}/{len(ipu_intervals)} transcrita: '{segment_text}'\")\n",
    "        except Exception as e:\n",
    "            segment_text = \"\"\n",
    "            print(f\"[Error] Falló transcripción de IPU {i + 1}: {e}\")\n",
    "\n",
    "        aligned.append((start, end, segment_text))\n",
    "\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35f087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phonemes_espeak(text):\n",
    "    try:\n",
    "        cmd = f'espeak-ng -q -v es -x \"{text}\"'\n",
    "        result = subprocess.check_output(cmd, shell=True, encoding='utf-8')\n",
    "        return result.strip()\n",
    "    except subprocess.CalledProcessError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c84ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transcription_errors(audio_path, ipu_intervals, transc_intervals):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    errors = []\n",
    "\n",
    "    for ipu, transc in zip(ipu_intervals, transc_intervals):\n",
    "        start, end = ipu[0], ipu[1]\n",
    "        transc_text = transc[2].strip()\n",
    "\n",
    "        if not transc_text:\n",
    "            continue\n",
    "\n",
    "        ipu_text = transcribe_audio_segment(audio_path, start, end, model)\n",
    "\n",
    "        if not ipu_text:\n",
    "            continue\n",
    "\n",
    "        phon_ipu = get_phonemes_espeak(ipu_text)\n",
    "        phon_transc = get_phonemes_espeak(transc_text)\n",
    "\n",
    "        if phon_ipu != phon_transc:\n",
    "            errors.append((start, end, f\"Desajuste fonético\"))\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae8a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_errors_to_txt(errors, path_txt):\n",
    "    with open(path_txt, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Inicio\\tFin\\tDescripción\\n\")\n",
    "        for start, end, desc in errors:\n",
    "            f.write(f\"{start:.2f}\\t{end:.2f}\\t{desc}\\n\")\n",
    "    print(f\"[✓] Errores exportados a: {path_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092a1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_textgrid(ipu_textgrid_path, html_path, output_path, audio_path):\n",
    "    \n",
    "    tg = textgrid.openTextgrid(ipu_textgrid_path, includeEmptyIntervals=True)\n",
    "    ipu_tier = tg.getTier(\"IPU\")\n",
    "    ipu_intervals = ipu_tier.entries\n",
    "\n",
    "  \n",
    "    model = whisper.load_model(\"base\")\n",
    "    transc_entries = []\n",
    "    for start, end, _ in ipu_intervals:\n",
    "        segment_text = transcribe_audio_segment(audio_path, start, end, model)\n",
    "        transc_entries.append((start, end, segment_text.strip()))\n",
    "\n",
    "    \n",
    "    transc_tier = textgrid.IntervalTier(\"Transc\", transc_entries, tg.minTimestamp, tg.maxTimestamp)\n",
    "\n",
    "    \n",
    "    error_entries = find_transcription_errors(audio_path, ipu_intervals, transc_entries)\n",
    "    errors_tier = textgrid.IntervalTier(\"TranscErrors\", error_entries, tg.minTimestamp, tg.maxTimestamp)\n",
    "\n",
    "    \n",
    "    tg.addTier(transc_tier)\n",
    "    tg.addTier(errors_tier)\n",
    "\n",
    "    \n",
    "    tg.save(output_path, format=\"short_textgrid\", includeBlankSpaces=True)\n",
    "    print(f\"[✓] TextGrid completo guardado en: {output_path}\")\n",
    "\n",
    "    \n",
    "    export_errors_to_txt(error_entries, output_path.replace(\".TextGrid\", \"_errores.txt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de3ade",
   "metadata": {},
   "source": [
    "#### Generación de TextGrid, detección de IPUs y alineación tradicional (segmentos de audio + espeak-ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1e26205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔊 Procesando: 202003311300.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Transcripción guardada en: ./Resultados\\202003311300.html\n",
      "[✓] TextGrid IPU guardado en: ./Resultados\\202003311300.TextGrid\n",
      "[✓] TextGrid completo guardado en: ./Resultados\\202003311300_v.TextGrid\n",
      "[✓] Errores exportados a: ./Resultados\\202003311300_v_errores.txt\n"
     ]
    }
   ],
   "source": [
    "carpeta_audios = \"./wav\"\n",
    "carpeta_resultados = \"./Resultados\"\n",
    "\n",
    "for nombre_archivo in os.listdir(carpeta_audios):\n",
    "    if nombre_archivo.endswith(\".wav\"):\n",
    "        print(f\"\\n🔊 Procesando: {nombre_archivo}\")\n",
    "\n",
    "        ruta_audio = os.path.join(carpeta_audios, nombre_archivo)\n",
    "        nombre_base = os.path.splitext(nombre_archivo)[0]\n",
    "\n",
    "        ruta_html = os.path.join(carpeta_resultados, f\"{nombre_base}.html\")\n",
    "        ruta_textgrid = os.path.join(carpeta_resultados, f\"{nombre_base}.TextGrid\")\n",
    "        ruta_textgrid_procesado = os.path.join(carpeta_resultados, f\"{nombre_base}_v.TextGrid\")#Tier con Errores \n",
    "\n",
    "        try:\n",
    "           \n",
    "            transcribir_audio_a_html(ruta_audio, ruta_html)\n",
    "\n",
    "            \n",
    "            detectar_ipus(ruta_audio, ruta_textgrid)\n",
    "\n",
    "\n",
    "            process_textgrid(ruta_textgrid, ruta_html, ruta_textgrid_procesado, ruta_audio)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error procesando {nombre_archivo}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
